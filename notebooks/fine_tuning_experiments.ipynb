{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Experiments\n",
    "Interactive experiments for QLoRA fine-tuning and evaluation of Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import transformers, peft\n",
    "import trl\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path: sys.path.append(str(ROOT))\n",
    "from src.fine_tune import (create_bnb_config, create_lora_config, load_datasets, load_model_and_tokenizer, create_training_arguments, train_model, evaluate_model)\n",
    "from src.model_inference import load_inference_model\n",
    "print('Versions:', transformers.__version__, peft.__version__, trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "DATA_TRAIN = Path('data/processed/combined_train.jsonl')\n",
    "DATA_VAL = Path('data/processed/combined_val.jsonl')\n",
    "print('Train exists:', DATA_TRAIN.exists(), 'Val exists:', DATA_VAL.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect datasets\n",
    "try:\n",
    "    train_ds, val_ds = load_datasets(DATA_TRAIN, DATA_VAL)\n",
    "    print('Train size:', len(train_ds), 'Val size:', len(val_ds))\n",
    "    print('Sample:', train_ds[0])\n",
    "except Exception as e:\n",
    "    print('Dataset load error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configs\n",
    "bnb = create_bnb_config(use_qlora=True)\n",
    "lora = create_lora_config(r=16, alpha=32)\n",
    "bnb, lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model/tokenizer (may download ~14GB)\n",
    "# Uncomment to run\n",
    "# model, tokenizer = load_model_and_tokenizer(os.getenv('BASE_MODEL_NAME', 'mistralai/Mistral-7B-Instruct-v0.1'), bnb)\n",
    "# tokenizer.apply_chat_template([{'role':'user','content':'Hello!'}], tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training smoke test (configure tiny run)\n",
    "# Uncomment to execute a short run\n",
    "# args = create_training_arguments(Path('models/adapters'), 1, 1, 1, 2e-4, 10, 10, 2, 512)\n",
    "# trainer = train_model(model, tokenizer, train_ds.select(range(8)), val_ds.select(range(8)), lora, args)\n",
    "# evaluate_model(trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference demo (after training)\n",
    "# inf = load_inference_model(adapter_path='models/adapters/final', load_in_4bit=True)\n",
    "# msgs = [\n",
    "#     {'role':'system','content':'You are an educational tutor.'},\n",
    "#     {'role':'user','content':'Explain photosynthesis in simple terms.'}\n",
    "# ]\n",
    "# print(inf.generate(msgs, max_new_tokens=128, temperature=0.7))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
